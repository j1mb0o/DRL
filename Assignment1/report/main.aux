\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{sec:Dynamic}{{1}{1}{Introduction}{section.1}{}}
\newlabel{greedy Q}{{1}{1}{Introduction}{equation.1.1}{}}
\newlabel{Dynamic Q Value update}{{2}{1}{Introduction}{equation.1.2}{}}
\newlabel{Dynamic Programming}{{1}{1}{Introduction}{algorithm.1}{}}
\newlabel{fig:Initial State}{{1}{1}{Q-values at the beginning of the algorithm, each Q(s,a) is initialized with 0}{figure.1}{}}
\newlabel{fig:Final State}{{2}{1}{Q-values after the convergence of the Algorithm \ref {Dynamic Programming}, each state has its optimal action to choose}{figure.2}{}}
\newlabel{sec:Exploration}{{2}{2}{Introduction}{section.2}{}}
\newlabel{eq:Boltzmann Policy}{{4}{2}{Introduction}{equation.2.4}{}}
\newlabel{eq:backup Q learning}{{5}{2}{Introduction}{equation.2.5}{}}
\newlabel{eq:Q learning update}{{6}{2}{Introduction}{equation.2.6}{}}
\newlabel{alg:Tabular Q-learning}{{2}{2}{Introduction}{algorithm.2}{}}
\newlabel{fig:Exploration}{{3}{3}{Reward per timestep using multiple values for $\epsilon $ and $\tau $}{figure.3}{}}
\newlabel{sec:Policy}{{3}{3}{Introduction}{section.3}{}}
\newlabel{eq:Sarga G_t}{{7}{3}{Introduction}{equation.3.7}{}}
\newlabel{alg:Tabular SARSA}{{3}{3}{Introduction}{algorithm.3}{}}
\newlabel{fig:SARSA}{{4}{3}{Reward per timestep using multiple values a for both Q-learning and SARSA}{figure.4}{}}
\newlabel{eq:Depth Target G}{{9}{4}{Introduction}{equation.4.9}{}}
\newlabel{alg:Tabular n-step Q-learning}{{4}{4}{Introduction}{algorithm.4}{}}
\newlabel{alg:Tabular Monte Carlo reinforcement learning.}{{5}{4}{Introduction}{algorithm.5}{}}
\newlabel{fig:katastrofi}{{5}{4}{Average reward per timestep using 1,5,10 and 30 steps}{figure.5}{}}
\citation{sagar_2020}
\citation{Plaat_2022}
\bibdata{main}
\bibcite{Plaat_2022}{{1}{2022}{{Plaat}}{{}}}
\bibcite{sagar_2020}{{2}{2020}{{Sagar}}{{}}}
\bibstyle{icml2021}
\gdef \@abspage@last{5}
